{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pstanisl/mlprague-2021/blob/main/01_introduction_to_bandits.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7rBvsgMUflr"
      },
      "source": [
        "# MLPrague 2021\r\n",
        "\r\n",
        "## How to Make Data-Driven Decisions: The Case for Contextual Multi-armed Bandits\r\n",
        "\r\n",
        "### Petr Stanislav & Michal Pleva"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGhk35-c5UwV"
      },
      "source": [
        "## Difference between ML and RL\r\n",
        "\r\n",
        "The first category, **supervised learning**, is the one you may be most familiar with. It relies on the idea of creating a function or model based on a set of training data, which contains inputs and their corresponding labels. Convolutional Neural Networks are a great example of this, as the images are the inputs and the outputs are the classifications of the images (dog, cat, etc).\r\n",
        "\r\n",
        "**Unsupervised learning** seeks to find some sort of structure within data through methods of cluster analysis. One of the most well-known ML clustering algorithms, K-Means, is an example of unsupervised learning.\r\n",
        "\r\n",
        "**Reinforcement learning** is the task of learning what actions to take, given a certain situation/environment, so as to maximize a reward signal. The interesting difference between supervised and reinforcement learning is that this reward signal simply tells you whether the action (or input) that the agent takes is good or bad. It doesn‚Äôt tell you anything about what the best action is. Contrast this to CNNs where the corresponding label for each image input is a definite instruction of what the output should be for each input.  Another unique component of RL is that an agent‚Äôs actions will affect the subsequent data it receives. For example, an agent‚Äôs action of moving left instead of right means that the agent will receive different input from the environment at the next time step. Let‚Äôs look at an example to start off.\r\n",
        "\r\n",
        "## What is the magic behind RL?\r\n",
        "\r\n",
        "Reinforcement learning (RL) is a general framework where agents learn to perform actions in an environment to maximize a reward. The two main components are the **environment**, which represents the problem to be solved (real world), and the **agent**, representing the learning algorithm.\r\n",
        "\r\n",
        "The agent and environment continuously interact with each other. See picture below. At each time step, the agent takes an action on the environment based on its policy $\\pi(a_{t}|s_{t})$, where ùë†ùë° is the current observation from the environment and receives a reward $r_{t+1}$ and the next observation $s_{t+1}$ from the environment. **The goal is to improve the policy so as to maximize the sum of rewards (return).**\r\n",
        "\r\n",
        "The \"magic\" is hiding in the fact that the agent is learning from the received rewards. Compared to standard machine learning methods (mainly supervised learning), we do not need to know all possible outcomes.\r\n",
        "\r\n",
        "<center>\r\n",
        "  <img src=\"https://miro.medium.com/max/1400/1*7cuAqjQ97x1H_sBIeAVVZg.png\" alt=\"rl-principle\" width=600 />\r\n",
        "</center>\r\n",
        "\r\n",
        "For better understanding is to important define some key terms that describe the basic elements of an RL problem are:\r\n",
        "\r\n",
        "* Environment ‚Äî Physical world in which the agent operates;\r\n",
        "* State ‚Äî Current situation of the agent;\r\n",
        "* Reward ‚Äî Feedback from the environment;\r\n",
        "* Policy ‚Äî Method to map agent‚Äôs state to actions;\r\n",
        "* Value ‚Äî Future reward that an agent would receive by taking an action in a particular state;\r\n",
        "\r\n",
        "## Few examples from real life\r\n",
        "\r\n",
        "- *A master chess player* makes a move. The choice is informed both by planning-anticipating possible replies and counterreplies and by immediate, intuitive judgments of the desirability of particular positions and moves.\r\n",
        "- *An adaptive controller* adjusts parameters of a petroleum refinery‚Äôs operation in real time. The controller optimizes the yield/cost/quality trade-off on the basis of specified marginal costs without sticking strictly to the set points originally suggested by engineers.\r\n",
        "- *A mobile robot* decides whether it should enter a new room in search of more trash to collect or start trying to find its way back to its battery recharging station. It makes its decision based on the current charge level of its battery and how quickly and easily it has been able to find the recharger in the past.\r\n",
        "- *A stock trader* decides what to do with his shares. State represents, how many shares of each stock I own, what is the current price of each stock and how much cash we have (uninvested). He has three possibilities of actions - sell, buy or hold. Reward is a change in value of portfolio from one step to the next.\r\n",
        "\r\n",
        "## Multi-Armed Bandits\r\n",
        "\r\n",
        "Multi-Armed Bandit (MAB) is a Machine Learning framework in which an agent has to select actions (arms) in order to maximize its cumulative reward in the long term. In each round, the agent receives some information about the current state (context), then it chooses an action based on this information and the experience gathered in previous rounds. At the end of each round, the agent receives the reward assiociated with the chosen action.\r\n",
        "\r\n",
        "Perhaps the purest example is the problem that lent its name to MAB: imagine that we are faced with $k$ slot machines (one-armed bandits), and we need to figure out which one has the best payout, while not losing too much money.\r\n",
        "\r\n",
        "<center>\r\n",
        "  <img src=\"https://blogs.mathworks.com/images/loren/2016/multiarmedbandit.jpg\" alt=\"slot machines\" />\r\n",
        "</center>\r\n",
        "\r\n",
        "Trying each machine once and then choosing the one that paid the most would not be a good strategy: The agent could fall into choosing a machine that had a lucky outcome in the beginning but is suboptimal in general. Instead, the agent should repeatedly come back to choosing machines that do not look so good, in order to collect more information about them. This is the main challenge in Multi-Armed Bandits: the agent has to find the right mixture between exploiting prior knowledge and exploring so as to avoid overlooking the optimal actions.\r\n",
        "\r\n",
        "More practical instances of MAB involve a piece of side information every time the learner makes a decision. We call this side information \"context\" or \"observation\".\r\n",
        "\r\n",
        "\r\n",
        "### How it relates to Multi-Armed Bandits?\r\n",
        "\r\n",
        "In the general RL case, the next observation $s_{t+1}$ depends on the previous state $s_{t}$ and the action $a_{t}$ taken by the policy. This last part is what separates MAB from RL: in MAB, the next state, which is the observation, does not depend on the action chosen by the agent.\r\n",
        "\r\n",
        "This similarity allows us to solve it with the same principle used in general RL.\r\n",
        "\r\n",
        "* An **environment** outputs observations, and responds to actions with rewards.\r\n",
        "* A **policy** outputs an action based on an observation, and\r\n",
        "* An **agent** repeatedly updates the policy based on previous observation-action-reward tuples.\r\n",
        "\r\n",
        "### Standard way how we think about MAB\r\n",
        "\r\n",
        "In traditional A/B testing methodologies, traffic is evenly split between two variations (both get 50%). Multi-armed bandits allow us to dynamically allocate traffic to variations that are performing well while allocating less and less traffic to underperforming variations. Multi-armed bandits are known to produce faster results since there‚Äôs no need to wait for a single winning variation. With the help of a relevant user data stream, multi-armed bandits can become context-based. Contextual bandit algorithms rely on an incoming stream of user context data, either historical or fresh, which can be used to make better algorithmic decisions in real-time.\r\n",
        "\r\n",
        "![](https://www.dynamicyield.com/wp-content/uploads/2015/02/AB-vs-Bandit-Testing.png)\r\n",
        "\r\n",
        "> Most of the description was kindly borrowed from [Tenserflow Agents overview](https://www.tensorflow.org/agents/overview)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4F8sGL_zdh8"
      },
      "source": [
        "## Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1hhU7FFCUUGA"
      },
      "outputs": [],
      "source": [
        "from typing import Dict, List, Tuple\r\n",
        "\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvtqpajRwz3o"
      },
      "source": [
        "Example of the Greedy Multi-Armed Bandit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ela0DXyswzO8"
      },
      "outputs": [],
      "source": [
        "class BannerEnvironment(object):\n",
        "  \"\"\"Example of environment for banners (with Bernoulli distribution of CTR)\"\"\"\n",
        "  def __init__(self, params: List[float]):\n",
        "    self._params = params\n",
        "    self._observe()\n",
        "\n",
        "  def reset(self):\n",
        "    return self._observe()\n",
        "\n",
        "  def _observe(self) -> List[float]:\n",
        "    self._observation = np.random.rand(1)\n",
        "    return self._observation\n",
        "\n",
        "  def step(self, action: int) -> Tuple[int, float]:\n",
        "    ret = 0 if self._observe()[0] > self._params[action] else 1\n",
        "    return (ret, self._observation[0])\n",
        "  \n",
        "  def best_action(self):\n",
        "    return np.argmax(self._params)\n",
        "\n",
        "\n",
        "class GreedyPolicy(object):\n",
        "  \"\"\"Simple greedy policy\"\"\"\n",
        "\n",
        "  def __init__(self, values):\n",
        "    self._values = values\n",
        "\n",
        "  def action(self) -> int:\n",
        "    return np.argmax(self._values)\n",
        "\n",
        "\n",
        "class GreedyAgent(object):\n",
        "  \"\"\"Greedy Agent with optimistic initialization.\"\"\"\n",
        "\n",
        "  def __init__(self, n: int):\n",
        "    self._n = n\n",
        "\n",
        "    self.reset()\n",
        "\n",
        "    self.policy = GreedyPolicy(self._values)\n",
        "\n",
        "  def reset(self):\n",
        "    self._counts = [0] * self._n\n",
        "    self._values = [1.0] * self._n\n",
        "\n",
        "  def train(self, experience: Dict[str, int]):\n",
        "    action = experience['action']\n",
        "    reward = experience['reward']\n",
        "\n",
        "    self._counts[action] += 1\n",
        "\n",
        "    value = self._values[action]\n",
        "    n = self._counts[action]\n",
        "\n",
        "    self._values[action] = ((n - 1) / n) * value + (1 / n) * reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DTbAHiFGx9AD",
        "outputId": "66773834-f1d5-45b5-a86f-6bd90700e751"
      },
      "outputs": [
        {
          "data": {
            "application/x.notebook.stream": "Agent's reward estimations=[0.0, 0.5, 0.6082474226804119] and counts=[1, 2, 97]\n"
          },
          "output_type": "unknown"
        }
      ],
      "source": [
        "environment = BannerEnvironment([0.25, 0.4, 0.67])\n",
        "environment.reset()\n",
        "\n",
        "agent = GreedyAgent(3)\n",
        "\n",
        "for _ in range(100):\n",
        "  action = agent.policy.action()  \n",
        "  reward, _= environment.step(action) \n",
        "  # Create trajectory nested \n",
        "  experience = {'action': action, 'reward': reward}\n",
        "  # Train policy in the agent\n",
        "  agent.train(experience)\n",
        "\n",
        "print(f'Agent\\'s reward estimations={agent._values} and counts={agent._counts}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ku2Qu7o3ye2O"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "name": "01_introduction_to_bandits.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {}
  },
  "nbformat": 4,
  "nbformat_minor": 0
}