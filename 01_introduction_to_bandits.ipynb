{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "01_introduction_to_bandits.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pstanisl/mlprague-2021/blob/main/01_introduction_to_bandits.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7rBvsgMUflr"
      },
      "source": [
        "# MLPrague 2021\r\n",
        "## How to Make Data-Driven Decisions: The Case for Contextual Multi-armed Bandits\r\n",
        "### Petr Stanislav & Michal Pleva"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMuZQJJoUseA"
      },
      "source": [
        "# Introduction\r\n",
        "\r\n",
        "The Multi-Armed Bandit problem (MAB) is a special case of Reinforcement Learning: an agent collects rewards in an environment by taking some actions after observing some state of the environment. The main difference between general RL and MAB is that in MAB, we assume that the action taken by the agent does not influence the next state of the environment. Therefore, agents do not model state transitions, credit rewards to past actions, or \"plan ahead\" to get to reward-rich states.\r\n",
        "\r\n",
        "As in other RL domains, the goal of a MAB agent is to find a policy that collects as much reward as possible. It would be a mistake, however, to always try to exploit the action that promises the highest reward, because then there is a chance that we miss out on better actions if we do not explore enough. This is the main problem to be solved in (MAB), often called the exploration-exploitation dilemma.\r\n",
        "\r\n",
        "Bandit environments, policies, and agents for MAB can be found in subdirectories of [tf_agents/bandits](https://github.com/tensorflow/agents/tree/master/tf_agents/bandits)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4F8sGL_zdh8"
      },
      "source": [
        "## Example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hhU7FFCUUGA"
      },
      "source": [
        "from typing import Dict, List, Tuple\r\n",
        "\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import numpy as np"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvtqpajRwz3o"
      },
      "source": [
        "Example of the Greedy Multi-Armed Bandit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ela0DXyswzO8"
      },
      "source": [
        "class BannerEnvironment(object):\n",
        "  \"\"\"Example of environment for banners (with Bernoulli distribution of CTR)\"\"\"\n",
        "  def __init__(self, params: List[float]):\n",
        "    self._params = params\n",
        "    self._observe()\n",
        "\n",
        "  def reset(self):\n",
        "    return self._observe()\n",
        "\n",
        "  def _observe(self) -> List[float]:\n",
        "    self._observation = np.random.rand(1)\n",
        "    return self._observation\n",
        "\n",
        "  def step(self, action: int) -> Tuple[int, float]:\n",
        "    ret = 0 if self._observe()[0] > self._params[action] else 1\n",
        "    return (ret, self._observation[0])\n",
        "  \n",
        "  def best_action(self):\n",
        "    return np.argmax(self._params)\n",
        "\n",
        "\n",
        "class GreedyPolicy(object):\n",
        "  \"\"\"Simple greedy policy\"\"\"\n",
        "\n",
        "  def __init__(self, values):\n",
        "    self._values = values\n",
        "\n",
        "  def action(self) -> int:\n",
        "    return np.argmax(self._values)\n",
        "\n",
        "\n",
        "class GreedyAgent(object):\n",
        "  \"\"\"Greedy Agent with optimistic initialization.\"\"\"\n",
        "\n",
        "  def __init__(self, n: int):\n",
        "    self._n = n\n",
        "\n",
        "    self.reset()\n",
        "\n",
        "    self.policy = GreedyPolicy(self._values)\n",
        "\n",
        "  def reset(self):\n",
        "    self._counts = [0] * self._n\n",
        "    self._values = [1.0] * self._n\n",
        "\n",
        "  def train(self, experience: Dict[str, int]):\n",
        "    action = experience['action']\n",
        "    reward = experience['reward']\n",
        "\n",
        "    self._counts[action] += 1\n",
        "\n",
        "    value = self._values[action]\n",
        "    n = self._counts[action]\n",
        "\n",
        "    self._values[action] = ((n - 1) / n) * value + (1 / n) * reward"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DTbAHiFGx9AD",
        "outputId": "66773834-f1d5-45b5-a86f-6bd90700e751",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "environment = BannerEnvironment([0.25, 0.4, 0.67])\n",
        "environment.reset()\n",
        "\n",
        "agent = GreedyAgent(3)\n",
        "\n",
        "for _ in range(100):\n",
        "  action = agent.policy.action()  \n",
        "  reward, _= environment.step(action) \n",
        "  # Create trajectory nested \n",
        "  experience = {'action': action, 'reward': reward}\n",
        "  # Train policy in the agent\n",
        "  agent.train(experience)\n",
        "\n",
        "print(f'Agent\\'s reward estimations={agent._values} and counts={agent._counts}')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Agent's reward estimations=[0.0, 0.5, 0.6082474226804119] and counts=[1, 2, 97]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ku2Qu7o3ye2O"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}