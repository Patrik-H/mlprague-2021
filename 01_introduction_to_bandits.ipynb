{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "01_introduction_to_bandits.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNWj+hMn0Ya7glg2iDl+Ef+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pstanisl/mlprague-2021/blob/main/01_introduction_to_bandits.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7rBvsgMUflr"
      },
      "source": [
        "# MLPrague 2021\r\n",
        "## How to Make Data-Driven Decisions: The Case for Contextual Multi-armed Bandits\r\n",
        "### Petr Stanislav & Michal Pleva"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMuZQJJoUseA"
      },
      "source": [
        "# Introduction\r\n",
        "\r\n",
        "The Multi-Armed Bandit problem (MAB) is a special case of Reinforcement Learning: an agent collects rewards in an environment by taking some actions after observing some state of the environment. The main difference between general RL and MAB is that in MAB, we assume that the action taken by the agent does not influence the next state of the environment. Therefore, agents do not model state transitions, credit rewards to past actions, or \"plan ahead\" to get to reward-rich states.\r\n",
        "\r\n",
        "As in other RL domains, the goal of a MAB agent is to find a policy that collects as much reward as possible. It would be a mistake, however, to always try to exploit the action that promises the highest reward, because then there is a chance that we miss out on better actions if we do not explore enough. This is the main problem to be solved in (MAB), often called the exploration-exploitation dilemma.\r\n",
        "\r\n",
        "Bandit environments, policies, and agents for MAB can be found in subdirectories of [tf_agents/bandits](https://github.com/tensorflow/agents/tree/master/tf_agents/bandits)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hhU7FFCUUGA"
      },
      "source": [
        "from typing import Dict, List, Tuple\r\n",
        "\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import seaborn as sns\r\n",
        "\r\n",
        "# Apply the default theme\r\n",
        "sns.set_theme()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}