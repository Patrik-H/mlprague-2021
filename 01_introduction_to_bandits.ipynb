{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "01_introduction_to_bandits.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pstanisl/mlprague-2021/blob/main/01_introduction_to_bandits.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7rBvsgMUflr"
      },
      "source": [
        "# MLPrague 2021\r\n",
        "\r\n",
        "## How to Make Data-Driven Decisions: The Case for Contextual Multi-armed Bandits\r\n",
        "\r\n",
        "### Petr Stanislav & Michal Pleva"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGhk35-c5UwV"
      },
      "source": [
        "<!-- ## Introduction\n",
        "\n",
        "The Multi-Armed Bandit problem (MAB) is a special case of Reinforcement Learning: an agent collects rewards in an environment by taking some actions after observing some state of the environment. The main difference between general RL and MAB is that in MAB, we assume that the action taken by the agent does not influence the next state of the environment. Therefore, agents do not model state transitions, credit rewards to past actions, or \"plan ahead\" to get to reward-rich states.\n",
        "\n",
        "As in other RL domains, the goal of a MAB agent is to find a policy that collects as much reward as possible. It would be a mistake, however, to always try to exploit the action that promises the highest reward, because then there is a chance that we miss out on better actions if we do not explore enough. This is the main problem to be solved in (MAB), often called the exploration-exploitation dilemma.\n",
        "\n",
        "Bandit environments, policies, and agents for MAB can be found in subdirectories of [tf_agents/bandits](https://github.com/tensorflow/agents/tree/master/tf_agents/bandits). \n",
        "-->\n",
        "## What is the magic behind RL?\n",
        "\n",
        "Reinforcement learning (RL) is a general framework where agents learn to perform actions in an environment to maximize a reward. The two main components are the **environment**, which represents the problem to be solved (real world), and the **agent**, representing the learning algorithm.\n",
        "\n",
        "The agent and environment continuously interact with each other. See picture below. At each time step, the agent takes an action on the environment based on its policy $\\pi(a_{t}|s_{t})$, where ùë†ùë° is the current observation from the environment and receives a reward $r_{t+1}$ and the next observation $s_{t+1}$ from the environment. **The goal is to improve the policy so as to maximize the sum of rewards (return).**\n",
        "\n",
        "The \"magic\" is hiding in the fact that the agent is learning from the received rewards. Compared to standard machine learning methods (mainly supervised learning), we do not need to know all possible outcomes.\n",
        "\n",
        "<center>\n",
        "  <img src=\"https://miro.medium.com/max/1400/1*7cuAqjQ97x1H_sBIeAVVZg.png\" alt=\"rl-principle\" width=600 />\n",
        "</center>\n",
        "\n",
        "For better understanding is to important define some key terms that describe the basic elements of an RL problem are:\n",
        "\n",
        "* Environment ‚Äî Physical world in which the agent operates;\n",
        "* State ‚Äî Current situation of the agent;\n",
        "* Reward ‚Äî Feedback from the environment;\n",
        "* Policy ‚Äî Method to map agent‚Äôs state to actions;\n",
        "* Value ‚Äî Future reward that an agent would receive by taking an action in a particular state;\n",
        "\n",
        "## Multi-Armed Bandits\n",
        "\n",
        "Multi-Armed Bandit (MAB) is a Machine Learning framework in which an agent has to select actions (arms) in order to maximize its cumulative reward in the long term. In each round, the agent receives some information about the current state (context), then it chooses an action based on this information and the experience gathered in previous rounds. At the end of each round, the agent receives the reward assiociated with the chosen action.\n",
        "\n",
        "Perhaps the purest example is the problem that lent its name to MAB: imagine that we are faced with $k$ slot machines (one-armed bandits), and we need to figure out which one has the best payout, while not losing too much money.\n",
        "\n",
        "<center>\n",
        "  <img src=\"https://blogs.mathworks.com/images/loren/2016/multiarmedbandit.jpg\" alt=\"slot machines\" />\n",
        "</center>\n",
        "\n",
        "Trying each machine once and then choosing the one that paid the most would not be a good strategy: The agent could fall into choosing a machine that had a lucky outcome in the beginning but is suboptimal in general. Instead, the agent should repeatedly come back to choosing machines that do not look so good, in order to collect more information about them. This is the main challenge in Multi-Armed Bandits: the agent has to find the right mixture between exploiting prior knowledge and exploring so as to avoid overlooking the optimal actions.\n",
        "\n",
        "More practical instances of MAB involve a piece of side information every time the learner makes a decision. We call this side information \"context\" or \"observation\".\n",
        "\n",
        "\n",
        "### How it relates to Multi-Armed Bandits?\n",
        "\n",
        "In the general RL case, the next observation $s_{t+1}$ depends on the previous state $s_{t}$ and the action $a_{t}$ taken by the policy. This last part is what separates MAB from RL: in MAB, the next state, which is the observation, does not depend on the action chosen by the agent.\n",
        "\n",
        "This similarity allows us to solve it with the same principle used in general RL.\n",
        "\n",
        "* An **environment** outputs observations, and responds to actions with rewards.\n",
        "* A **policy** outputs an action based on an observation, and\n",
        "* An **agent** repeatedly updates the policy based on previous observation-action-reward tuples.\n",
        "\n",
        "### Standard way how we think about MAB\n",
        "\n",
        "In traditional A/B testing methodologies, traffic is evenly split between two variations (both get 50%). Multi-armed bandits allow us to dynamically allocate traffic to variations that are performing well while allocating less and less traffic to underperforming variations. Multi-armed bandits are known to produce faster results since there‚Äôs no need to wait for a single winning variation. With the help of a relevant user data stream, multi-armed bandits can become context-based. Contextual bandit algorithms rely on an incoming stream of user context data, either historical or fresh, which can be used to make better algorithmic decisions in real-time.\n",
        "\n",
        "![](https://www.dynamicyield.com/wp-content/uploads/2015/02/AB-vs-Bandit-Testing.png)\n",
        "\n",
        "> Most of the description was kindly borrowed from [Tenserflow Agents overview](https://www.tensorflow.org/agents/overview)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4F8sGL_zdh8"
      },
      "source": [
        "## Example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hhU7FFCUUGA"
      },
      "source": [
        "from typing import Dict, List, Tuple\r\n",
        "\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import numpy as np"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvtqpajRwz3o"
      },
      "source": [
        "Example of the Greedy Multi-Armed Bandit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ela0DXyswzO8"
      },
      "source": [
        "class BannerEnvironment(object):\n",
        "  \"\"\"Example of environment for banners (with Bernoulli distribution of CTR)\"\"\"\n",
        "  def __init__(self, params: List[float]):\n",
        "    self._params = params\n",
        "    self._observe()\n",
        "\n",
        "  def reset(self):\n",
        "    return self._observe()\n",
        "\n",
        "  def _observe(self) -> List[float]:\n",
        "    self._observation = np.random.rand(1)\n",
        "    return self._observation\n",
        "\n",
        "  def step(self, action: int) -> Tuple[int, float]:\n",
        "    ret = 0 if self._observe()[0] > self._params[action] else 1\n",
        "    return (ret, self._observation[0])\n",
        "  \n",
        "  def best_action(self):\n",
        "    return np.argmax(self._params)\n",
        "\n",
        "\n",
        "class GreedyPolicy(object):\n",
        "  \"\"\"Simple greedy policy\"\"\"\n",
        "\n",
        "  def __init__(self, values):\n",
        "    self._values = values\n",
        "\n",
        "  def action(self) -> int:\n",
        "    return np.argmax(self._values)\n",
        "\n",
        "\n",
        "class GreedyAgent(object):\n",
        "  \"\"\"Greedy Agent with optimistic initialization.\"\"\"\n",
        "\n",
        "  def __init__(self, n: int):\n",
        "    self._n = n\n",
        "\n",
        "    self.reset()\n",
        "\n",
        "    self.policy = GreedyPolicy(self._values)\n",
        "\n",
        "  def reset(self):\n",
        "    self._counts = [0] * self._n\n",
        "    self._values = [1.0] * self._n\n",
        "\n",
        "  def train(self, experience: Dict[str, int]):\n",
        "    action = experience['action']\n",
        "    reward = experience['reward']\n",
        "\n",
        "    self._counts[action] += 1\n",
        "\n",
        "    value = self._values[action]\n",
        "    n = self._counts[action]\n",
        "\n",
        "    self._values[action] = ((n - 1) / n) * value + (1 / n) * reward"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DTbAHiFGx9AD",
        "outputId": "66773834-f1d5-45b5-a86f-6bd90700e751"
      },
      "source": [
        "environment = BannerEnvironment([0.25, 0.4, 0.67])\n",
        "environment.reset()\n",
        "\n",
        "agent = GreedyAgent(3)\n",
        "\n",
        "for _ in range(100):\n",
        "  action = agent.policy.action()  \n",
        "  reward, _= environment.step(action) \n",
        "  # Create trajectory nested \n",
        "  experience = {'action': action, 'reward': reward}\n",
        "  # Train policy in the agent\n",
        "  agent.train(experience)\n",
        "\n",
        "print(f'Agent\\'s reward estimations={agent._values} and counts={agent._counts}')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Agent's reward estimations=[0.0, 0.5, 0.6082474226804119] and counts=[1, 2, 97]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ku2Qu7o3ye2O"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}